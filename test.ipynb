{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-21T11:05:26.081342Z",
     "start_time": "2025-01-21T11:05:24.868969Z"
    }
   },
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import helpers as hp\n",
    "\n",
    "# use same data every time\n",
    "X = np.loadtxt('data/in.txt')\n",
    "Y = np.loadtxt('data/out.txt')"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T11:10:15.501346Z",
     "start_time": "2025-01-21T11:10:15.236593Z"
    }
   },
   "cell_type": "code",
   "source": [
    "NumLayers=3 # does not count input-layer (but does count output)\n",
    "LayerSizes=[2,1,2,1] # input-layer,hidden-1,hidden-2,...,output-layer\n",
    "\n",
    "# initialize random weights and biases for all layers (except input of course)\n",
    "Weights=[np.random.uniform(low=-1,high=+1,size=[ LayerSizes[j],LayerSizes[j+1] ]) for j in range(NumLayers)]\n",
    "Biases=[np.random.uniform(low=-1,high=+1,size=LayerSizes[j+1]) for j in range(NumLayers)]\n",
    "\n",
    "# define the batchsize\n",
    "batchsize=len(X)\n",
    "eta=.001\n",
    "batches=10\n",
    "costs=[]\n",
    "\n",
    "for k in range(batches):\n",
    "    #x,y_target=make_batch(batchsize)\n",
    "    y_out_result,cost,Weights,Biases=hp.train_batch(X.T,Y.T,eta, Weights,Biases, batchsize)\n",
    "    costs.append(cost)\n",
    "    \n",
    "plt.plot(costs)\n",
    "plt.title(\"Cost function during training\")\n",
    "plt.show()\n",
    "    \n",
    "\n"
   ],
   "id": "2a9eebfd88fc9f62",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (2,1) and (2,2500) not aligned: 1 (dim 1) != 2 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 16\u001B[0m\n\u001B[0;32m     12\u001B[0m costs\u001B[38;5;241m=\u001B[39m[]\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(batches):\n\u001B[0;32m     15\u001B[0m     \u001B[38;5;66;03m#x,y_target=make_batch(batchsize)\u001B[39;00m\n\u001B[1;32m---> 16\u001B[0m     y_out_result,cost,Weights,Biases\u001B[38;5;241m=\u001B[39m\u001B[43mhp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mT\u001B[49m\u001B[43m,\u001B[49m\u001B[43mY\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mT\u001B[49m\u001B[43m,\u001B[49m\u001B[43meta\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mWeights\u001B[49m\u001B[43m,\u001B[49m\u001B[43mBiases\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatchsize\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     17\u001B[0m     costs\u001B[38;5;241m.\u001B[39mappend(cost)\n\u001B[0;32m     19\u001B[0m plt\u001B[38;5;241m.\u001B[39mplot(costs)\n",
      "File \u001B[1;32mD:\\dev\\tubs\\CompPhySheet3\\helpers.py:94\u001B[0m, in \u001B[0;36mtrain_batch\u001B[1;34m(x, y_target, eta, Weights, Biases, batchsize)\u001B[0m\n\u001B[0;32m     89\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtrain_batch\u001B[39m(x, y_target, eta, Weights, Biases, batchsize):  \u001B[38;5;66;03m# one full training batch\u001B[39;00m\n\u001B[0;32m     90\u001B[0m     \u001B[38;5;66;03m# x is an array of size batchsize x (input-layer-size)\u001B[39;00m\n\u001B[0;32m     91\u001B[0m     \u001B[38;5;66;03m# y_target is an array of size batchsize x (output-layer-size)\u001B[39;00m\n\u001B[0;32m     92\u001B[0m     \u001B[38;5;66;03m# eta is the stepsize for the gradient descent\u001B[39;00m\n\u001B[1;32m---> 94\u001B[0m     y_out_result, y_layer, df_layer \u001B[38;5;241m=\u001B[39m \u001B[43mapply_net\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mWeights\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mBiases\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     95\u001B[0m     dw_layer, db_layer \u001B[38;5;241m=\u001B[39m backprop(y_target, y_layer, df_layer, Weights, Biases)\n\u001B[0;32m     96\u001B[0m     Weights, Biases \u001B[38;5;241m=\u001B[39m gradient_step(eta, dw_layer, db_layer, Weights, Biases)\n",
      "File \u001B[1;32mD:\\dev\\tubs\\CompPhySheet3\\helpers.py:43\u001B[0m, in \u001B[0;36mapply_net\u001B[1;34m(x, Weights, Biases)\u001B[0m\n\u001B[0;32m     40\u001B[0m y_layer\u001B[38;5;241m.\u001B[39mappend(y)\n\u001B[0;32m     41\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m w, b \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(Weights, Biases):  \u001B[38;5;66;03m# loop through all layers\u001B[39;00m\n\u001B[0;32m     42\u001B[0m     \u001B[38;5;66;03m# j=0 corresponds to the first layer above the input\u001B[39;00m\n\u001B[1;32m---> 43\u001B[0m     y, df \u001B[38;5;241m=\u001B[39m \u001B[43mforward_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mw\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# one step\u001B[39;00m\n\u001B[0;32m     44\u001B[0m     df_layer\u001B[38;5;241m.\u001B[39mappend(df)  \u001B[38;5;66;03m# store f'(z) [needed later in backprop]\u001B[39;00m\n\u001B[0;32m     45\u001B[0m     y_layer\u001B[38;5;241m.\u001B[39mappend(y)  \u001B[38;5;66;03m# store f(z) [also needed in backprop]\u001B[39;00m\n",
      "File \u001B[1;32mD:\\dev\\tubs\\CompPhySheet3\\helpers.py:33\u001B[0m, in \u001B[0;36mforward_step\u001B[1;34m(y, w, b, activation)\u001B[0m\n\u001B[0;32m     32\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward_step\u001B[39m(y,w,b, activation\u001B[38;5;241m=\u001B[39mrelu):\n\u001B[1;32m---> 33\u001B[0m     z \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdot\u001B[49m\u001B[43m(\u001B[49m\u001B[43mw\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m+\u001B[39m b\n\u001B[0;32m     34\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m activation(z)\n",
      "\u001B[1;31mValueError\u001B[0m: shapes (2,1) and (2,2500) not aligned: 1 (dim 1) != 2 (dim 0)"
     ]
    }
   ],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
