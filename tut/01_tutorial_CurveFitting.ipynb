{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FlorianMarquardt/machine-learning-for-physicists/blob/master/2024/01_tutorial_CurveFitting.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonlinear curve fitting via stochastic gradient descent\n",
    "\n",
    "Example code for the lecture series \"Machine Learning for Physicists\" by Florian Marquardt\n",
    "\n",
    "Lecture 1, 2024 Tutorials\n",
    "\n",
    "This notebook shows how stochastic gradient descent can help fit an arbitrary function (neural networks essentially do the same, but in much higher dimensions and with many more parameters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import all necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.dpi']=300 # highres display\n",
    "\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define the target function to be fitted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define here the target function as $f(x) = \\dfrac{3}{(x-0.5)^2 +1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_f(x):\n",
    "    return( 3.0/((x-0.5)**2+1.0) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define the parametrized nonlinear function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we consider a parametrized function $F(x) [\\theta] = \\dfrac{\\theta_0}{(x - \\theta_1)^2+1}$, with a vector $\\theta$ of parameters $\\theta = (\\theta_0, \\theta_1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(theta,x):\n",
    "    \"\"\"\n",
    "    theta are the parameters\n",
    "    x are the input values (can be an array)\n",
    "    \"\"\"\n",
    "    return(theta[0]/((x-theta[1])**2+1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define the gradients of the function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We analytically calculate the derivative of $f[\\theta]$ with respect to its parameters: \n",
    "\n",
    "$\\partial f / \\partial \\theta_0 = \\dfrac{1}{(x - \\theta_1)^2+1}$ \n",
    "\n",
    "and \n",
    "\n",
    "$\\partial f / \\partial \\theta_1 = \\dfrac{2 (x - \\theta_1) \\theta_0}{[(x - \\theta_1)^2+1]^2}$.\n",
    "\n",
    "The gradient vector is defined as $\\nabla_\\theta f[\\theta] = (\\partial f / \\partial \\theta_0, \\partial f / \\partial \\theta_1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_grad(theta,x):\n",
    "    \"\"\"\n",
    "    Return the gradient of f with respect to theta\n",
    "    shape [n_theta,n_samples]\n",
    "    where n_theta=len(theta)\n",
    "    and n_samples=len(x)\n",
    "    \"\"\"\n",
    "    grad0 = 1./((x-theta[1])**2+1.0)\n",
    "    grad1 = 2*(x-theta[1])*theta[0]/((x-theta[1])**2+1.0)**2\n",
    "\n",
    "    return(np.array([grad0, grad1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Perform the parameters' optimization procedure with stochastic gradient descent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get randomly sampled x values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def samples(nsamples,width):\n",
    "    return(width*np.random.randn(nsamples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the average cost function on a grid of 2 parameters\n",
    "\n",
    "$c (\\theta) = \\langle C (x, \\theta) \\rangle_x$\n",
    "\n",
    "with \n",
    "\n",
    "$C (x, \\theta) = \\left( F(x)[\\theta] - f(x) \\right)^2 / 2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_cost(theta0s,theta1s,nsamples, width):\n",
    "    n0=len(theta0s)\n",
    "    n1=len(theta1s)\n",
    "    C=np.zeros([n0,n1])\n",
    "    for j0 in range(n0):\n",
    "        for j1 in range(n1):\n",
    "            theta=np.array([theta0s[j0],theta1s[j1]])\n",
    "            x=samples(nsamples,width)\n",
    "            C[j0,j1]=0.5*np.average((f(theta,x)-true_f(x))**2)\n",
    "    return(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take arbitrary parameters as starting point\n",
    "theta=np.array([1.5,-2.3])\n",
    "\n",
    "x=samples(100,2.)\n",
    "# illustrate the parametrized function, at sampled points,\n",
    "# compare against actual function\n",
    "plt.scatter(x,f(theta,x),color=\"orange\")\n",
    "plt.scatter(x,true_f(x),color=\"blue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the cost function landscape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta0s=np.linspace(-3,6,40)\n",
    "theta1s=np.linspace(-2,3,40)\n",
    "C=get_avg_cost(theta0s,theta1s,10000, 2.)\n",
    "nlevels=20\n",
    "X,Y=np.meshgrid(theta0s,theta1s,indexing='ij')\n",
    "plt.contourf(X,Y,C,nlevels)\n",
    "plt.contour(X,Y,C,nlevels,colors=\"white\")\n",
    "plt.xlabel(\"theta_0\")\n",
    "plt.ylabel(\"theta_1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform stochastic gradient descent and visualize the progress and the function\n",
    "\n",
    "Start with random values $\\theta = (\\theta_0, \\theta_1)$ and updated them according to the averaged stochastic gradients:\n",
    "\n",
    "$\\theta^{(n+1)} = \\theta^{(n)} - \\eta \\ \\partial c(\\theta) / \\partial \\theta$ ,\n",
    "\n",
    "which results into \n",
    "\n",
    "$\\theta^{(n+1)} = \\theta^{(n)} - \\eta \\ \\langle ( F(x)[\\theta] - f(x) ) \\cdot \\nabla_\\theta F(x)[\\theta] \\rangle_x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take arbitrary parameters as starting point in a given range\n",
    "theta0_range = 1.\n",
    "theta1_range = 2.\n",
    "theta=np.array([theta0_range,theta1_range])*np.random.rand(2)\n",
    "\n",
    "eta=.2 # \"learning rate\" (gradient descent step size)\n",
    "nsamples=10 # stochastic x samples used per step\n",
    "nsteps=1000 # how many steps we take\n",
    "\n",
    "x_sweep=np.linspace(-4,4,300)\n",
    "xrange = 2.\n",
    "\n",
    "for n in range(nsteps):\n",
    "    \n",
    "    x=samples(nsamples, xrange) # get random samples\n",
    "\n",
    "    # deviation from true function (vector):\n",
    "    deviation=f(theta,x)-true_f(x)\n",
    "\n",
    "    # do one gradient descent step:\n",
    "    theta-=eta*np.average(deviation[None,:]*f_grad(theta,x),axis=1)\n",
    "\n",
    "\n",
    "    # Now: Plotting\n",
    "    # compare true function (blue) against\n",
    "    # parametrized function (orange)\n",
    "    # blue dots indicate random points where\n",
    "    # the true function was sampled in this step\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    fig,ax=plt.subplots(ncols=2,nrows=1,figsize=(8,2))\n",
    "    \n",
    "    nlevels=20\n",
    "    ax[0].contourf(X,Y,C,nlevels)\n",
    "    ax[0].contour(X,Y,C,nlevels,colors=\"white\")\n",
    "    ax[0].scatter([theta[0]],[theta[1]],color=\"orange\")\n",
    "    ax[0].set_xlim(theta0s[0],theta0s[-1])\n",
    "    ax[0].set_ylim(theta1s[0],theta1s[-1])\n",
    "    ax[0].set_xlabel(\"theta_0\")\n",
    "    ax[0].set_ylabel(\"theta_1\")    \n",
    "    \n",
    "    ax[1].plot(x_sweep,true_f(x_sweep),color=\"blue\")\n",
    "    ax[1].scatter(x,true_f(x),color=\"blue\")\n",
    "    ax[1].plot(x_sweep,f(theta,x_sweep),color=\"orange\")\n",
    "    ax[1].set_xlim(-4,4)\n",
    "    ax[1].set_ylim(0.0,4.0)\n",
    "    ax[1].set_xlabel(\"x\")\n",
    "    ax[1].set_ylabel(\"f\") \n",
    "    \n",
    "    plt.show()\n",
    "    sleep(0.3)\n",
    "    \n",
    "print(theta) #print the final fitted values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional activities:\n",
    "\n",
    "1. Implement a gradient descent algorithm which stops only when it well approximates the function within some tolerance.\n",
    "2. Find the minimum number of steps to reach convergence for different tolerance values.\n",
    "3. How does the learning rate affects the convergence speed? Make some tests. \n",
    "4. Implement a learning schedule for the learning rate, e.g.: $\\eta = \\eta_0 t_1/(t+t_1)$ with parameters $t_0, t_1$ and step index $t$.\n",
    "5. Try to optimize more than 2 parameters at once of a given function (you can re-use the same function of the given example). \n",
    "6. Try to fit a sinusoidal function, e.g.: $\\sin(\\omega x + x_0)$, where $\\omega$ and $x_0$ are the parameters to be optimized.\n",
    "7. Try to solve the problem at 5. creating a batch of training function with different random initial values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sinusoidal function fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_f(x):\n",
    "    return( np.sin(5.*x+1.0) ) \n",
    "\n",
    "def f(theta,x):\n",
    "    return( np.sin(theta[0]*x+theta[1]) )\n",
    "\n",
    "def f_grad(theta,x):\n",
    "    grad0 = x*np.cos(theta[0]*x+theta[1])\n",
    "    grad1 = np.ones(len(x))\n",
    "    return(np.array([grad0, grad1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def samples(nsamples, width):\n",
    "    return(width*np.random.randn(nsamples))\n",
    "\n",
    "def get_avg_cost(theta0s,theta1s,nsamples, width):\n",
    "    n0=len(theta0s)\n",
    "    n1=len(theta1s)\n",
    "    C=np.zeros([n0,n1])\n",
    "    for j0 in range(n0):\n",
    "        for j1 in range(n1):\n",
    "            theta=np.array([theta0s[j0],theta1s[j1]])\n",
    "            x=samples(nsamples, width)\n",
    "            C[j0,j1]=0.5*np.average((f(theta,x)-true_f(x))**2)\n",
    "    return(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get average cost function:\n",
    "theta0s=np.linspace(-10,10,100)\n",
    "theta1s=np.linspace(-2,2,40)\n",
    "C=get_avg_cost(theta0s,theta1s,10000, 2.)\n",
    "nlevels=20\n",
    "X,Y=np.meshgrid(theta0s,theta1s,indexing='ij')\n",
    "plt.contourf(X,Y,C,nlevels)\n",
    "plt.contour(X,Y,C,nlevels,colors=\"white\")\n",
    "plt.xlabel(\"theta_0\")\n",
    "plt.ylabel(\"theta_1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take arbitrary parameters as starting point\n",
    "theta=np.array([10.,2.])*np.random.rand(2)\n",
    "print('Initial values:', theta)\n",
    "\n",
    "eta=.2 # \"learning rate\" (gradient descent step size)\n",
    "nsamples=10 # stochastic x samples used per step\n",
    "nsteps=10000 # how many steps we take\n",
    "\n",
    "x_sweep=np.linspace(-4,4,300)\n",
    "xrange = 2.\n",
    "\n",
    "for n in range(nsteps):\n",
    "    x=samples(nsamples, xrange) # get random samples\n",
    "    # deviation from true function (vector):\n",
    "    deviation=f(theta,x)-true_f(x)\n",
    "    # do one gradient descent step:\n",
    "    theta-=eta*np.average(deviation[None,:]*f_grad(theta,x),axis=1)\n",
    "\n",
    "    # Now: Plotting\n",
    "    # compare true function (blue) against\n",
    "    # parametrized function (orange)\n",
    "    # blue dots indicate random points where\n",
    "    # the true function was sampled in this step\n",
    "    clear_output(wait=True)\n",
    "    fig,ax=plt.subplots(ncols=2,nrows=1,figsize=(8,2))\n",
    "    \n",
    "    nlevels=20\n",
    "    ax[0].contourf(X,Y,C,nlevels)\n",
    "    ax[0].contour(X,Y,C,nlevels,colors=\"white\")\n",
    "    ax[0].scatter([theta[0]],[theta[1]],color=\"orange\")\n",
    "    ax[0].set_xlim(theta0s[0],theta0s[-1])\n",
    "    ax[0].set_ylim(theta1s[0],theta1s[-1])\n",
    "    ax[0].set_xlabel(\"theta_0\")\n",
    "    ax[0].set_ylabel(\"theta_1\")    \n",
    "    \n",
    "    ax[1].plot(x_sweep,true_f(x_sweep),color=\"blue\")\n",
    "    ax[1].scatter(x,true_f(x),color=\"blue\")\n",
    "    ax[1].plot(x_sweep,f(theta,x_sweep),color=\"orange\")\n",
    "    ax[1].set_xlim(-4,4)\n",
    "    ax[1].set_ylim(-2.0,2.0)\n",
    "    ax[1].set_xlabel(\"x\")\n",
    "    ax[1].set_ylabel(\"f\") \n",
    "    \n",
    "    plt.show()\n",
    "    sleep(0.3)\n",
    "    \n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Curve fitting with 3 parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_f(x):\n",
    "    return( 3.0/((x-0.5)**2+1.0) )\n",
    "\n",
    "def f(theta,x):\n",
    "    return(theta[0]/((x-theta[1])**2+theta[2]))\n",
    "\n",
    "def f_grad(theta,x):\n",
    "    grad0 = 1./((x-theta[1])**2+theta[2])\n",
    "    grad1 = 2*(x-theta[1])*theta[0]/((x-theta[1])**2+theta[2])**2\n",
    "    grad2 = -theta[0]/((x-theta[1])**2+theta[2])**2\n",
    "\n",
    "    return(np.array([grad0, grad1, grad2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def samples(nsamples,width):\n",
    "    return(width*np.random.randn(nsamples))\n",
    "    \n",
    "def get_avg_cost(theta0s,theta1s, nsamples, width):\n",
    "    n0=len(theta0s)\n",
    "    n1=len(theta1s)\n",
    "    C0=np.zeros([n0,n1])\n",
    "    C1=np.zeros([n0,n1])\n",
    "    for j0 in range(n0):\n",
    "        for j1 in range(n1):\n",
    "            theta=np.array([theta0s[j0],theta1s[j1],1.])\n",
    "            x=samples(nsamples, width)\n",
    "            C0[j0,j1]=0.5*np.average((f(theta,x)-true_f(x))**2)\n",
    "    for j0 in range(n0):\n",
    "        for j1 in range(n1):\n",
    "            theta=np.array([theta0s[j0],0.5,theta1s[j1]])\n",
    "            x=samples(nsamples, width)\n",
    "            C1[j0,j1]=0.5*np.average((f(theta,x)-true_f(x))**2)\n",
    "    return(C0, C1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get average cost function:\n",
    "theta0s=np.linspace(-3,6,40)\n",
    "theta1s=np.linspace(-2,3,40)\n",
    "theta2s=np.linspace(0.5,1.5,40)\n",
    "C0,_=get_avg_cost(theta0s,theta1s,10000, 2.)\n",
    "_,C1=get_avg_cost(theta0s,theta2s,10000, 2.)\n",
    "nlevels=20\n",
    "X,Y=np.meshgrid(theta0s,theta1s,indexing='ij')\n",
    "X,Z=np.meshgrid(theta0s,theta2s,indexing='ij')\n",
    "\n",
    "plt.contourf(X,Z,C1,nlevels)\n",
    "plt.contour(X,Z,C1,nlevels,colors=\"white\")\n",
    "plt.xlabel(\"theta_0\")\n",
    "plt.ylabel(\"theta_2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_plot = False\n",
    "\n",
    "# take arbitrary parameters as starting point\n",
    "theta=np.array([-1.0,2.0,0.4])\n",
    "\n",
    "eta=.2 # \"learning rate\" (gradient descent step size)\n",
    "nsamples=10 # stochastic x samples used per step\n",
    "nsteps=10000 # how many steps we take\n",
    "\n",
    "tolerance = 0.05\n",
    "\n",
    "x_sweep=np.linspace(-4,4,400)\n",
    "xrange = 2.\n",
    "\n",
    "for n in range(nsteps):\n",
    "    x=samples(nsamples, xrange) \n",
    "    deviation=f(theta,x)-true_f(x)\n",
    "    theta-=eta*np.average(deviation[None,:]*f_grad(theta,x),axis=1)\n",
    "\n",
    "    \n",
    "    if make_plot:\n",
    "        clear_output(wait=True)\n",
    "        fig,ax=plt.subplots(ncols=3,nrows=1,figsize=(12,2))\n",
    "        \n",
    "        nlevels=20\n",
    "        ax[0].contourf(X,Y,C0,nlevels)\n",
    "        ax[0].contour(X,Y,C0,nlevels,colors=\"white\")\n",
    "        ax[0].scatter([theta[0]],[theta[1]],color=\"orange\")\n",
    "        ax[0].set_xlim(theta0s[0],theta0s[-1])\n",
    "        ax[0].set_ylim(theta1s[0],theta1s[-1])\n",
    "        ax[0].set_xlabel(\"theta_0\")\n",
    "        ax[0].set_ylabel(\"theta_1\")    \n",
    "        \n",
    "        ax[1].contourf(X,Z,C1,nlevels)\n",
    "        ax[1].contour(X,Z,C1,nlevels,colors=\"white\")\n",
    "        ax[1].scatter([theta[0]],[theta[2]],color=\"orange\")\n",
    "        ax[1].set_xlim(theta0s[0],theta0s[-1])\n",
    "        ax[1].set_ylim(theta2s[0],theta2s[-1])\n",
    "        ax[1].set_xlabel(\"theta_0\")\n",
    "        ax[1].set_ylabel(\"theta_2\") \n",
    "\n",
    "        ax[2].plot(x_sweep,true_f(x_sweep),color=\"blue\")\n",
    "        ax[2].scatter(x,true_f(x),color=\"blue\")\n",
    "        ax[2].plot(x_sweep,f(theta,x_sweep),color=\"orange\")\n",
    "        ax[2].set_xlim(-4,4)\n",
    "        ax[2].set_ylim(0.0,4.0)\n",
    "        ax[2].set_xlabel(\"x\")\n",
    "        ax[2].set_ylabel(\"f\") \n",
    "        \n",
    "        plt.show()\n",
    "        sleep(0.3)\n",
    "\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_plot = False\n",
    "\n",
    "theta=np.array([-1.0,2.0,0.4])\n",
    "\n",
    "eta=.2 # \"learning rate\" (gradient descent step size)\n",
    "nsamples=10 # stochastic x samples used per step\n",
    "\n",
    "tolerance = 1E-3\n",
    "i = 0\n",
    "x_sweep=np.linspace(-4,4,400)\n",
    "xrange = 2.\n",
    "\n",
    "fidelity = 10. \n",
    "while fidelity > tolerance:\n",
    "    x=samples(nsamples, xrange) # get random samples\n",
    "    fidelity=np.average((f(theta,x)-true_f(x))**2./2.)\n",
    "    deviation=f(theta,x)-true_f(x)\n",
    "    theta-=eta*np.average(deviation[None,:]*f_grad(theta,x),axis=1)\n",
    "    \n",
    "    i+=1\n",
    "\n",
    "    if make_plot:\n",
    "        clear_output(wait=True)\n",
    "        fig,ax=plt.subplots(ncols=3,nrows=1,figsize=(12,2))\n",
    "        \n",
    "        nlevels=20\n",
    "        ax[0].contourf(X,Y,C0,nlevels)\n",
    "        ax[0].contour(X,Y,C0,nlevels,colors=\"white\")\n",
    "        ax[0].scatter([theta[0]],[theta[1]],color=\"orange\")\n",
    "        ax[0].set_xlim(theta0s[0],theta0s[-1])\n",
    "        ax[0].set_ylim(theta1s[0],theta1s[-1])\n",
    "        ax[0].set_xlabel(\"theta_0\")\n",
    "        ax[0].set_ylabel(\"theta_1\")    \n",
    "        \n",
    "        ax[1].contourf(X,Z,C1,nlevels)\n",
    "        ax[1].contour(X,Z,C1,nlevels,colors=\"white\")\n",
    "        ax[1].scatter([theta[0]],[theta[2]],color=\"orange\")\n",
    "        ax[1].set_xlim(theta0s[0],theta0s[-1])\n",
    "        ax[1].set_ylim(theta2s[0],theta2s[-1])\n",
    "        ax[1].set_xlabel(\"theta_0\")\n",
    "        ax[1].set_ylabel(\"theta_2\") \n",
    "\n",
    "        ax[2].plot(x_sweep,true_f(x_sweep),color=\"blue\")\n",
    "        ax[2].scatter(x,true_f(x),color=\"blue\")\n",
    "        ax[2].plot(x_sweep,f(theta,x_sweep),color=\"orange\")\n",
    "        ax[2].set_xlim(-4,4)\n",
    "        ax[2].set_ylim(0.0,4.0)\n",
    "        ax[2].set_xlabel(\"x\")\n",
    "        ax[2].set_ylabel(\"f\") \n",
    "        \n",
    "        plt.show()\n",
    "        sleep(0.3)\n",
    "\n",
    "print('Set of parameters:',theta)\n",
    "print('Final fidelity:', fidelity)\n",
    "print('Number of steps:', i)\n",
    "\n",
    "plt.plot(x_sweep,true_f(x_sweep),color=\"blue\")\n",
    "plt.plot(x_sweep,f(theta,x_sweep),color=\"orange\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
